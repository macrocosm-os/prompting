{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miner Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a miner, read the miner.md located in assets/miner.md \n",
    "This notebook will demonstrate how to evaluate the results from testnet and compare it to expected results on mainnet.\n",
    "Please ensure that your miner has been running for at least 2 hours before evaluating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT DATA HERE\n",
    "YOUR_TESTNET_UID = 10\n",
    "TESTNET_RUN_ID = None\n",
    "MAINNET_RUN_ID = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate your run on testnet\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "api = wandb.Api(timeout=120)\n",
    "project_name = \"macrocosmos/prompting-validators\"\n",
    "if TESTNET_RUN_ID is None:\n",
    "    tag = \"Netuid: 61\"\n",
    "    # Fetch all runs from the project\n",
    "    runs = api.runs(project_name)\n",
    "\n",
    "    # Filter runs that contain the specific tag\n",
    "    filtered_runs = [run for run in runs if tag in run.tags]\n",
    "\n",
    "    # Sort runs by created time, with the latest first\n",
    "    if filtered_runs:\n",
    "        latest_run_testnet = sorted(filtered_runs, key=lambda run: run.created_at, reverse=True)[0]\n",
    "    else:\n",
    "        print(\"No runs found with the specified tag. Try to manually locate the run using the Weights and Biases web app.\")\n",
    "        print(\"If there is no active run, please message us in discord and we will spin one up for you.\")\n",
    "else:\n",
    "    latest_run_testnet = api.run(f\"{project_name}/{TESTNET_RUN_ID}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAINNET_RUN_ID is None:\n",
    "    # Pull the most recent run from mainnet by taostats\n",
    "    tag = \"Netuid: 1\"\n",
    "    username = \"taostats\"\n",
    "\n",
    "    # Fetch all runs from the project\n",
    "    runs = api.runs(project_name)\n",
    "\n",
    "    # Filter runs that contain the specific tag and are from the specified user\n",
    "    filtered_runs = [run for run in runs if tag in run.tags and run.state == \"running\"]\n",
    "\n",
    "    # Sort runs by created time, with the latest first\n",
    "    if filtered_runs:\n",
    "        latest_run_mainnet = sorted(filtered_runs, key=lambda run: run.created_at, reverse=True)[0]\n",
    "    else:\n",
    "        print(\"No runs found with the specified tag. Try to manually locate the most recent taostats run on mainnet using the Weights and Biases web app.\")\n",
    "else:\n",
    "    latest_run_mainnet = api.run(f\"{project_name}/{MAINNET_RUN_ID}\")\n",
    "Â¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testnet_df = pd.DataFrame(latest_run_testnet.scan_history()).explode(['uids', 'rewards'])\n",
    "mainnet_df = pd.DataFrame(latest_run_mainnet.history()).explode(['uids', 'rewards'])\n",
    "\n",
    "testnet_df = testnet_df[testnet_df['uids'] == YOUR_TESTNET_UID].tail(100)\n",
    "\n",
    "grouped_testnet_df = testnet_df.groupby('task').rewards.mean()\n",
    "grouped_mainnet_df = mainnet_df.groupby('task').rewards.mean()\n",
    "\n",
    "# Plot the rewards grouped by task for both testnet and mainnet\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(grouped_testnet_df.index, grouped_testnet_df, label='Testnet Rewards', alpha=0.3)\n",
    "plt.bar(grouped_mainnet_df.index, grouped_mainnet_df, label='Mainnet Rewards', alpha=0.3)\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Rewards Comparison: Testnet vs Mainnet')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vllm\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"casperhansen/qwen2-0.5b-instruct-awq\", gpu_memory_utilization=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/qwen2-0.5b-instruct-awq\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"casperhansen/qwen2-0.5b-instruct-awq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(model=\"casperhansen/llama-3-8b-instruct-awq\", gpu_memory_utilization=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/prompting-fb5sw-i7-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 12:24:41,043\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-28 12:24:41 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 10-28 12:24:41 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='casperhansen/llama-3-8b-instruct-awq', speculative_config=None, tokenizer='casperhansen/llama-3-8b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/llama-3-8b-instruct-awq, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 10-28 12:24:42 model_runner.py:680] Starting to load model casperhansen/llama-3-8b-instruct-awq...\n",
      "INFO 10-28 12:24:42 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  4.88it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.66it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.84it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-28 12:24:44 model_runner.py:692] Loading model weights took 5.3453 GB\n",
      "INFO 10-28 12:24:45 gpu_executor.py:102] # GPU blocks: 7844, # CPU blocks: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.23it/s, est. speed input: 17.08 toks/s, output: 68.30 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='Who are you?', prompt_token_ids=[15546, 527, 499, 30], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Arenâ€™t you just some quote-unquote reporter wasting my time?â€\\nI held', token_ids=(74604, 1431, 499, 1120, 1063, 12929, 20486, 3022, 19496, 48897, 856, 892, 12671, 198, 40, 5762), cumulative_logprob=-43.68973688001279, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1730118289.7709765, last_token_time=1730118289.7709765, first_scheduled_time=1730118289.7766387, first_token_time=1730118289.8125596, time_in_queue=0.005662202835083008, finished_time=1730118290.0105848), lora_request=None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "from vllm import LLM\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"casperhansen/llama-3-8b-instruct-awq\",\n",
    "    tensor_parallel_size=1,\n",
    "    pipeline_parallel_size=1,\n",
    "    disable_custom_all_reduce=True,\n",
    "    swap_space=4,\n",
    "    max_num_batched_tokens=4096,\n",
    "    trust_remote_code=False,\n",
    "    max_model_len=2048,\n",
    "    dtype=\"float16\",\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=0.5,\n",
    ")\n",
    "llm.generate(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.82it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI'm LLaMA, a large language model developed by Meta AI that can understand and respond to human input in a conversational manner. I'm trained on a massive dataset of text from the internet and can generate human-like responses to a wide range of topics and questions.\\n\\nI'm not a human, but rather a computer program designed to simulate conversation and answer questions to the best of my ability. My training data includes a vast amount of text from various sources, including books, articles, and websites, which allows me to learn about and generate responses to a wide range of topics.\\n\\nI'm constantly learning and improving my performance, so the more you chat with me, the better I'll become at understanding and responding to your questions and statements! What would you like to talk about?\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"casperhansen/llama-3-8b-instruct-awq\", max_length=200, device=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/llama-3-8b-instruct-awq\")\n",
    "messages_formatted = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "pipe(messages_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"1/ ðŸŒŸ What is Radix?Radix s a cutting-edge decentralized platform designed to defeat the biggest challenges in blockchain: scalability, security, and ease of use. But what sets it apart? Let's explore! #DeFi #Blockchain  #RadixDLT ðŸ‘‘\\nRadix solves some of the most pressing issues in blockchain, such as:\\nScalability: Radix employs a novel architecture, using shards and a distributed ledger, to achieve incredible scalability, capable of processing 10,000+ transactions per second!\\nSecurity: Radix boasts an adaptive, multi-layered security framework, ensuring unparalleled protection for your assets and data.\\nEase of use: Radix boasts an intuitive interface, streamlining the process of joining, creating, and managing your own blockchain-based dApps.\\nAnd that's not all! ðŸ¤¯ Radix has also made significant strides in:\\nCost reduction: Radix's energy-efficient design reduces costs by up to 90% compared to traditional\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"1/ ðŸŒŸ What is Radix?Radix s a cutting-edge decentralized platform designed to defeat the biggest challenges in blockchain: scalability, security, and ease of use. But what sets it apart? Let's explore! #DeFi #Blockchain  #RadixDLT ðŸ‘‘\\nRadix solves some of the most pressing issues in blockchain, such as:\\nScalability: Radix employs a novel architecture, using shards and a distributed ledger, to achieve incredible scalability, capable of processing 10,000+ transactions per second!\\nSecurity: Radix boasts an adaptive, multi-layered security framework, ensuring unparalleled protection for your assets and data.\\nEase of use: Radix boasts an intuitive interface, streamlining the process of joining, creating, and managing your own blockchain-based dApps.\\nAnd that's not all! ðŸ¤¯ Radix has also made significant strides in:\\nCost reduction: Radix's energy-efficient design reduces costs by up to 90% compared to traditional\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"1/ ðŸŒŸ What is Radix?Radix s a cutting-edge decentralized platform designed to defeat the biggest challenges in blockchain: scalability, security, and ease of use. But what sets it apart? Let's explore! #DeFi #Blockchain  #RadixDLT ðŸ‘‘\\nRadix solves some of the most pressing issues in blockchain, such as:\\nScalability: Radix employs a novel architecture, using shards and a distributed ledger, to achieve incredible scalability, capable of processing 10,000+ transactions per second!\\nSecurity: Radix boasts an adaptive, multi-layered security framework, ensuring unparalleled protection for your assets and data.\\nEase of use: Radix boasts an intuitive interface, streamlining the process of joining, creating, and managing your own blockchain-based dApps.\\nAnd that's not all! ðŸ¤¯ Radix has also made significant strides in:\\nCost reduction: Radix's energy-efficient design reduces costs by up to 90% compared to traditional\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"1/ ðŸŒŸ What is Radix?Radix s a cutting-edge decentralized platform designed to defeat the biggest challenges in blockchain: scalability, security, and ease of use. But what sets it apart? Let's explore! #DeFi #Blockchain  #RadixDLT ðŸ‘‘\\nRadix solves some of the most pressing issues in blockchain, such as:\\nScalability: Radix employs a novel architecture, using shards and a distributed ledger, to achieve incredible scalability, capable of processing 10,000+ transactions per second!\\nSecurity: Radix boasts an adaptive, multi-layered security framework, ensuring unparalleled protection for your assets and data.\\nEase of use: Radix boasts an intuitive interface, streamlining the process of joining, creating, and managing your own blockchain-based dApps.\\nAnd that's not all! ðŸ¤¯ Radix has also made significant strides in:\\nCost reduction: Radix's energy-efficient design reduces costs by up to 90% compared to traditional\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"1/ ðŸŒŸ What is Radix?Radix s a cutting-edge decentralized platform designed to defeat the biggest challenges in blockchain: scalability, security, and ease of use. But what sets it apart? Let's explore! #DeFi #Blockchain  #RadixDLT ðŸ‘‘\\nRadix solves some of the most pressing issues in blockchain, such as:\\nScalability: Radix employs a novel architecture, using shards and a distributed ledger, to achieve incredible scalability, capable of processing 10,000+ transactions per second!\\nSecurity: Radix boasts an adaptive, multi-layered security framework, ensuring unparalleled protection for your assets and data.\\nEase of use: Radix boasts an intuitive interface, streamlining the process of joining, creating, and managing your own blockchain-based dApps.\\nAnd that's not all! ðŸ¤¯ Radix has also made significant strides in:\\nCost reduction: Radix's energy-efficient design reduces costs by up to 90% compared to traditional\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"1/ ðŸŒŸ What is Radix?Radix s a cutting-edge decentralized platform designed to defeat the biggest challenges in blockchain: scalability, security, and ease of use. But what sets it apart? Let's explore! #DeFi #Blockchain  #RadixDLT ðŸ‘‘\\nRadix solves some of the most pressing issues in blockchain, such as:\\nScalability: Radix employs a novel architecture, using shards and a distributed ledger, to achieve incredible scalability, capable of processing 10,000+ transactions per second!\\nSecurity: Radix boasts an adaptive, multi-layered security framework, ensuring unparalleled protection for your assets and data.\\nEase of use: Radix boasts an intuitive interface, streamlining the process of joining, creating, and managing your own blockchain-based dApps.\\nAnd that's not all! ðŸ¤¯ Radix has also made significant strides in:\\nCost reduction: Radix's energy-efficient design reduces costs by up to 90% compared to traditional\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"1/ ðŸŒŸ What is Radix?Radix s a cutting-edge decentralized platform designed to defeat the biggest challenges in blockchain: scalability, security, and ease of use. But what sets it apart? Let's explore! #DeFi #Blockchain  #RadixDLT ðŸ‘‘\\nRadix solves some of the most pressing issues in blockchain, such as:\\nScalability: Radix employs a novel architecture, using shards and a distributed ledger, to achieve incredible scalability, capable of processing 10,000+ transactions per second!\\nSecurity: Radix boasts an adaptive, multi-layered security framework, ensuring unparalleled protection for your assets and data.\\nEase of use: Radix boasts an intuitive interface, streamlining the process of joining, creating, and managing your own blockchain-based dApps.\\nAnd that's not all! ðŸ¤¯ Radix has also made significant strides in:\\nCost reduction: Radix's energy-efficient design reduces costs by up to 90% compared to traditional\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"1/ ðŸŒŸ What is Radix?Radix s a cutting-edge decentralized platform designed to defeat the biggest challenges in blockchain: scalability, security, and ease of use. But what sets it apart? Let's explore! #DeFi #Blockchain  #RadixDLT ðŸ‘‘\\nRadix solves some of the most pressing issues in blockchain, such as:\\nScalability: Radix employs a novel architecture, using shards and a distributed ledger, to achieve incredible scalability, capable of processing 10,000+ transactions per second!\\nSecurity: Radix boasts an adaptive, multi-layered security framework, ensuring unparalleled protection for your assets and data.\\nEase of use: Radix boasts an intuitive interface, streamlining the process of joining, creating, and managing your own blockchain-based dApps.\\nAnd that's not all! ðŸ¤¯ Radix has also made significant strides in:\\nCost reduction: Radix's energy-efficient design reduces costs by up to 90% compared to traditional\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"1/ ðŸŒŸ What is Radix?Radix s a cutting-edge decentralized platform designed to defeat the biggest challenges in blockchain: scalability, security, and ease of use. But what sets it apart? Let's explore! #DeFi #Blockchain  #RadixDLT ðŸ‘‘\\nRadix solves some of the most pressing issues in blockchain, such as:\\nScalability: Radix employs a novel architecture, using shards and a distributed ledger, to achieve incredible scalability, capable of processing 10,000+ transactions per second!\\nSecurity: Radix boasts an adaptive, multi-layered security framework, ensuring unparalleled protection for your assets and data.\\nEase of use: Radix boasts an intuitive interface, streamlining the process of joining, creating, and managing your own blockchain-based dApps.\\nAnd that's not all! ðŸ¤¯ Radix has also made significant strides in:\\nCost reduction: Radix's energy-efficient design reduces costs by up to 90% compared to traditional\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import SamplingParams\n",
    "import gc\n",
    "import transformers\n",
    "# Global variables for model and tokenizer\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"casperhansen/qwen2-0.5b-instruct-awq\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"casperhansen/qwen2-0.5b-instruct-awq\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set all seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    transformers.set_seed(seed)\n",
    "\n",
    "def generate(prompt, seed):\n",
    "    global model, tokenizer\n",
    "    \n",
    "    # Set all seeds\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Tokenize input\n",
    "    # inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # # Generate\n",
    "    # with torch.no_grad():\n",
    "    #     outputs = model.generate(\n",
    "    #         **inputs,\n",
    "    #         max_new_tokens=200,\n",
    "    #         do_sample=True,\n",
    "    #         temperature=0.01,\n",
    "    #         pad_token_id=tokenizer.pad_token_id,\n",
    "    #         eos_token_id=tokenizer.eos_token_id,\n",
    "    #     )\n",
    "    \n",
    "    # Decode and return\n",
    "    # return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return llm.generate(prompt, SamplingParams(seed=seed))[0].outputs[0].text\n",
    "    # return pipe(prompt)\n",
    "\n",
    "# Generate multiple times\n",
    "for _ in range(10):\n",
    "    result = generate(\n",
    "        \"1/ ðŸŒŸ What is Radix?Radix s a cutting-edge decentralized platform designed to defeat the biggest challenges in blockchain: scalability, security, and ease of use. But what sets it apart? Let's explore! #DeFi #Blockchain  #Radix\",\n",
    "        967594\n",
    "    )\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
