{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Overview\n",
    "\n",
    "## Summarization\n",
    "This consists of a query whereby the agent (Validator) requests a summary of a topic from the miners. Always uses API\n",
    "\n",
    "The context is from Wikipedia. The context is used for creating the reference and may or may not be also sent to the miners.\n",
    "\n",
    "1. Select wikipedia article at random and use to define TOPIC (e.g. Pearl Harbour) and CONTEXT (article content)\n",
    "2. Extract TAGS (history, WW2, Japan, USA) associated with article\n",
    "3. Generate SYSTEM PROMPT such as 'You are a student who wants a summary of the main events of TOPIC (TAGS) in a XYZ tone'.\n",
    "4. Generate QUERY using MODEL and SYSTEM PROMPT\n",
    "5. Generate K REFERENCES using MODEL with & without CONTEXT (helps us understand the efficacy of tool use in miners)\n",
    "6. Repeat step 5 using GPT and other models (e.g. mixtral, solar)\n",
    "\n",
    "----\n",
    "system_prompt = 'You are a student who want a summary of Pradeep Kumar Dubey (politics) in an interested tone.'\n",
    "\n",
    "system prompt is given to our agent (LLM) and the agent generates a query:\n",
    "\n",
    "query = 'Give me an overview of the politician Pradeep Kumar Dubey'\n",
    "query = 'Provide me with a summary of Pradeep Kumar Dubey'\n",
    "query = 'I want to know about Pradeep Kumar Dubey, can you give me a summary?'\n",
    "\n",
    "Query is then sent to the miners.\n",
    "\n",
    "\n",
    "\n",
    "## Question Answering\n",
    "This consists of a query whereby the agent (Validator) requests an answer to a question from the miners. Always uses API.\n",
    "\n",
    "## Debugging\n",
    "This can consist of either:\n",
    "- Non API: Reference answer (code snippet) provided by the agent, followed by a corruption step to create the challenge. Only a single reference answer exists\n",
    "- API: Stack overflow is used to find a random thread containing a question and one or more accepted/upvoted answers. In this case the reference answers are weighted by upvotes and the challenge is the user question. Multiple reference answers exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "import bittensor as bt\n",
    "\n",
    "import pandas as pd\n",
    "       \n",
    "from utils import load_llm\n",
    "from prompting.agent import Agent\n",
    "from prompting.tasks import DebuggingTask, QuestionAnsweringTask, SummarizationTask\n",
    "\n",
    "\n",
    "gpt_judge_prompt = \"\"\"I'm using a roleplaying AI assistant to imitate human queries. You task is to assess whether the following query follows the instruction correctly.  If the assistant-generated query contains system messages such as 'sure i can help' or similar, this is a bad result because humans would not talk to an AI assistant in that way.\n",
    "\n",
    "system_prompt = {system_prompt}\n",
    "\n",
    "query = {query}'\n",
    "\n",
    "Does the above query follow the system prompt and strongly resemble a human message? \n",
    "\n",
    "Simply answer 0 or 1, and your result must be enclosed in { } tags\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-4'\n",
    "llm = load_llm(model, api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def get_random_wikipedia_article(min_length=1000, min_backlinks=1):\n",
    "    # Wikipedia API endpoint for a random article\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'info|linkshere|categories|categoryinfo|extracts',\n",
    "        'generator': 'random',\n",
    "        'grnnamespace': 0,  # Namespace 0 indicates articles\n",
    "        'grnlimit': 10,     # Number of random articles to fetch\n",
    "        'inprop': 'url|displaytitle|length',  # Requesting URL, title, and length of the page\n",
    "        'lhprop': 'pageid',  # Properties for links here (backlinks)\n",
    "        'lhlimit': 'max',    # Maximum number of backlinks to retrieve\n",
    "        'exlimit': 'max',    # Get extracts for each page\n",
    "        'cllimit': 'max'     # Get all categories for each page\n",
    "    }\n",
    "\n",
    "    \n",
    "    max_tries = 10\n",
    "    tries = 0\n",
    "    while tries < max_tries:\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        tries += 1\n",
    "        \n",
    "        data = response.json()\n",
    "        if not data.get('query'):\n",
    "            continue\n",
    "\n",
    "        for page_id, page_info in data['query']['pages'].items():\n",
    "\n",
    "            length = page_info.get('length', 0)\n",
    "            backlinks = len(page_info.get('linkshere', []))\n",
    "            categories = [cat.get('title','').strip('Category:') for cat in page_info.get('categories', [{}])]\n",
    "            extract = page_info.get('extract')\n",
    "                \n",
    "            if length >= min_length and backlinks >= min_backlinks and extract:# and views >= min_views:\n",
    "                return {\n",
    "                    'title': page_info['title'],\n",
    "                    'url': page_info['fullurl'],\n",
    "                    'length': length,\n",
    "                    'extract': extract,\n",
    "                    'backlinks': backlinks,\n",
    "                    'categories': categories\n",
    "                }\n",
    "    raise Exception(f\"Could not find an article with length >= {min_length} and backlinks >= {min_backlinks} after {max_tries} tries.\")\n",
    "\n",
    "# Example usage\n",
    "filtered_data = get_random_wikipedia_article()\n",
    "filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_random_wikipedia_article():\n",
    "    # Wikipedia API endpoint for a random article\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'info|linkshere|categories|categoryinfo|pageviews',#|extracts\n",
    "        'generator': 'random',\n",
    "        'grnnamespace': 0,  # Namespace 0 indicates articles\n",
    "        'grnlimit': 20       # Number of random articles to fetch\n",
    "    }\n",
    "\n",
    "    # Making the API request\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "    # Extracting the title of the random article\n",
    "    title = data['query']['random'][0]['title']\n",
    "\n",
    "    # URL of the random article\n",
    "    article_url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "\n",
    "    return title, article_url\n",
    "\n",
    "get_random_wikipedia_article()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_wikipedia_article_content(title, remove_headers=False):\n",
    "    # Wikipedia API endpoint\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request to get article content\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': title,\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': True,  # Get plain text content\n",
    "    }\n",
    "\n",
    "    # Making the API request\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting the page content\n",
    "    page = next(iter(data['query']['pages'].values()))\n",
    "    content = page.get('extract', 'Content not found.')\n",
    "    \n",
    "    text = ''\n",
    "    for line in content.split('\\n'):\n",
    "        if remove_headers and line.startswith('==') and line.endswith('=='):\n",
    "            continue\n",
    "        text += line + '\\n'\n",
    "\n",
    "    return text\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "# TODO: maybe this?\n",
    "def extract_categories(url):\n",
    "    # Fetch the webpage\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the category links\n",
    "    categories = []\n",
    "    for link in soup.find_all(\"a\", href=lambda href: href and \"Category:\" in href):\n",
    "        category = link.get_text()\n",
    "        categories.append(category)\n",
    "\n",
    "    return categories\n",
    "\n",
    "# Assuming you have a title from the previous function\n",
    "title, url = get_random_wikipedia_article()\n",
    "content = get_wikipedia_article_content(title, remove_headers=True)\n",
    "categories = extract_categories(url)\n",
    "print(f\"Title: {title}\\nContent:\\n{content}\\nCategories: {categories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['query']['pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "\n",
    "def get_pages_in_category(category):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'list': 'categorymembers',\n",
    "        'cmtitle': f\"Category:{category}\",\n",
    "        'cmnamespace': 0,  # Specify namespace 0 for articles\n",
    "        'cmlimit': 'max'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    page_titles = [page['title'] for page in data['query']['categorymembers']]\n",
    "    return page_titles\n",
    "\n",
    "\n",
    "def select_random_page(pages):\n",
    "    if pages:\n",
    "        return random.choice(pages)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example: Fetch a random page from the \"Physics\" category\n",
    "category = \"Science\"\n",
    "for i in range(3):\n",
    "    pages = get_pages_in_category(category)\n",
    "    print(f'Category: {category}. Content: {pages}')\n",
    "    random_page_title = category = select_random_page(pages)\n",
    "    print(f\"Random page from {category} category: {random_page_title}. Total pages: {len(pages)}\")\n",
    "\n",
    "print(f\"Random page from {category} category: {random_page_title}. Total pages: {len(pages)}\")\n",
    "# content = get_wikipedia_article_content(random_page_title, remove_headers=False)\n",
    "# print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_top_level_categories(category=\"Contents\"):\n",
    "    # Wikipedia API endpoint\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request to get subcategories\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'list': 'categorymembers',\n",
    "        'cmtitle': f\"Category:{category}\",\n",
    "        'cmtype': 'subcat',  # Fetch subcategories\n",
    "        'cmlimit': 'max'     # Maximum number of subcategories\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    subcategories = [subcat['title'].replace(\"Category:\", \"\") for subcat in data['query']['categorymembers']]\n",
    "    return subcategories\n",
    "\n",
    "# Fetch top-level categories\n",
    "top_level_categories = get_top_level_categories(category='History')\n",
    "print(f\"Top-level Categories:\\n{top_level_categories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "\n",
    "def get_pages_in_category(category):\n",
    "    # Encode spaces for URL\n",
    "    category = category.replace(' ', '_')\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'list': 'categorymembers',\n",
    "        'cmtitle': f\"Category:{category}\",\n",
    "        'cmnamespace': 0,\n",
    "        'cmlimit': 'max'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    page_titles = [page['title'] for page in data['query']['categorymembers']]\n",
    "    return page_titles\n",
    "\n",
    "def get_random_wikipedia_article(categories):\n",
    "    # Select a random category\n",
    "    selected_category = random.choice(categories)\n",
    "    print(f\"Selected Category: {selected_category}\")\n",
    "\n",
    "    # Get pages in the selected category\n",
    "    pages = get_pages_in_category(selected_category)\n",
    "    if not pages:\n",
    "        raise ValueError( \"No articles found in the category.\" )\n",
    "\n",
    "    # Select a random page\n",
    "    random_page_title = random.choice(pages)\n",
    "    print(f\"Selected Article: {random_page_title}\")\n",
    "\n",
    "    # Get the content of the random page\n",
    "    url = f\"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': random_page_title,\n",
    "        'prop': 'extracts',\n",
    "        'explaintext': True,\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    page = next(iter(data['query']['pages'].values()))\n",
    "    content = page.get('extract', 'Content not found.')\n",
    "\n",
    "    # Construct the article URL\n",
    "    article_url = f\"https://en.wikipedia.org/wiki/{random_page_title.replace(' ', '_')}\"\n",
    "\n",
    "    return {'title': random_page_title, 'url': article_url, 'content': content, 'category': selected_category}\n",
    "\n",
    "# List of categories\n",
    "categories = ['Artificial intelligence', 'World history', 'Astrophysics', 'Classical music', \n",
    "              'Environmental science', 'Food', 'Mythology', 'Contemporary art', 'Linguistics']\n",
    "\n",
    "categories = ['Machine learning algorithms']\n",
    "# Get a random article\n",
    "results = []\n",
    "import tqdm\n",
    "for i in tqdm.tqdm(range(100)):\n",
    "    try:\n",
    "        data = get_random_wikipedia_article(categories)\n",
    "        results.append(data)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "    \n",
    "print(f\"Title: {title}\\nURL: {url}\\nContent: {content[:500]}...\")  # Print the first 500 characters of content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).title.value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections.abc import Iterator\n",
    "\n",
    "class Dataset(Iterator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __next__(self):\n",
    "        max_tries = 20\n",
    "        while True:\n",
    "            \n",
    "            bt.logging.debug(\"Retrieving data from prompting.dataset...\")\n",
    "            for _ in range(max_tries):\n",
    "                title, url = get_random_wikipedia_article()\n",
    "                content = get_wikipedia_article_content(title)\n",
    "                \n",
    "                if f'{title} may refer to:' in content:\n",
    "                    continue\n",
    "\n",
    "                if len(content.split()) < 250:\n",
    "                    continue\n",
    "                \n",
    "                # TBD\n",
    "                tags = []\n",
    "\n",
    "                # TODO return useful addition fields\n",
    "                if content.strip():\n",
    "                    return {\"text\": content,'title': title, 'url': url, 'tags': tags}\n",
    "            \n",
    "            bt.logging.debug(f\"Failed to retrieve data from prompting.dataset after {max_tries} tries\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_trials = 1\n",
    "n_references = 3\n",
    "\n",
    "tasks = [SummarizationTask(llm=llm, dataset=dataset)]#, DebuggingTask, QuestionAnsweringTask]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i in range(n_trials):\n",
    "\n",
    "    # loop over all task types\n",
    "    # for now, just summarization\n",
    "    for task in tasks:\n",
    "        \n",
    "        # loop over all the different formulations of a given task\n",
    "        # for task_params in [{}]:\n",
    "            \n",
    "        # If this is debugging a reference has been created, otherwise there is no reference\n",
    "        # For now we will ignore debugging tasks for simplicity\n",
    "        # task = task_class(llm, **task_params)\n",
    "        \n",
    "        bt.logging.info(\"ðŸ¤– Creating agent...\")\n",
    "        agent = Agent(llm=llm, task=task)\n",
    "        \n",
    "        query = agent.query\n",
    "        # Create reference answers\n",
    "        bt.logging.info(\"ðŸ¤– Creating reference answers...\")\n",
    "        references = agent.generate_reference_answers(n=n_references)\n",
    "        \n",
    "        query_eval = GPT(gpt_judge_prompt).parse()\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(task.challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( task.create_summary_prompt(task.challenge) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
