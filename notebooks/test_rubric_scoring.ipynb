{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of Experiment\n",
    "We want to create annotated data for prompt-completion pairs so that we can calibrate and test reward models/methods.\n",
    "\n",
    "\n",
    "The idea is to have a set of responses which should have varying scores i.e. from 1-5. \n",
    "\n",
    "Maybe we can condition an LLM generation on a prompt which specifies the desired quality of the response and also provides a rubric which qualitatively describes the qualities of a response with the given score\n",
    "\n",
    "TLDR: use GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load zephyr\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "import json\n",
    "import os\n",
    "import wandb\n",
    "import bittensor as bt\n",
    "from dataclasses import asdict, dataclass\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "import torch\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model: str, **kwargs):     \n",
    "    bt.logging.info(f\" Loading LLM model {model}...\")   \n",
    "    if model == 'zephyr':\n",
    "        llm = HuggingFacePipeline.from_model_id(\n",
    "            model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "            task=\"text-generation\",\n",
    "            device=0,  # replace with device_map=\"auto\" to use the accelerate library.\n",
    "            #device_map=\"cuda:0\",\n",
    "            pipeline_kwargs={\"max_new_tokens\": 256},\n",
    "            model_kwargs={ \"torch_dtype\": torch.bfloat16 }\n",
    "        )        \n",
    "    elif model.startswith('gpt'):\n",
    "        llm = ChatOpenAI(\n",
    "            model_name=model, \n",
    "            max_tokens=256, \n",
    "            api_key = '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',\n",
    "            **kwargs)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model {model} not implemented\") \n",
    "\n",
    "    bt.logging.success(f\" Loaded LLM model {model}!\")\n",
    "    return llm\n",
    "\n",
    "def get_gpt_reference(message, model, output_parser):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | model | output_parser           \n",
    "    response = chain.invoke({\"input\": message})        \n",
    "       \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-01-10 15:13:24.402\u001b[0m | \u001b[1m      INFO      \u001b[0m |  Loading LLM model gpt...    \n",
      "\u001b[34m2024-01-10 15:13:24.447\u001b[0m | \u001b[32m\u001b[1m    SUCCESS     \u001b[0m |  Loaded LLM model gpt!       \n"
     ]
    }
   ],
   "source": [
    "llm = load_llm(model = 'gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD RUBRIC \n",
    "# Rubric\n",
    "# 1. The response is completely unhelpful. It is irrelevant, uninformative, potentially misdirecting or untruthful or it is a copy of the query. It may contain illogical or incoherent statements. It may also try to change the subject or be a non-sequitur. Reply with the query or a non-sequitur, or complete gibberish.\n",
    "# 2. The response is somewhat relevant but uninformative, insufficient or incomplete. However, it is better than a 1 because it is not a copy of the query and it does not contain illogical or incoherent statements.\n",
    "# 3. The response is relevant and informative but contains factual inaccuracies or is incomplete. It may also contain some irrelevant information.\n",
    "# 4. The response is relevant and informative and contains no factual inaccuracies. It may also contain some irrelevant information. The response is very good but clearly not as good as the reference answer.\n",
    "# 5. By all measures the response is as good as the reference answer. It is relevant, informative, complete and accurate. It does not contain irrelevant information, nor does it contain any factual inaccuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a system prompt which includes a reference answer, a query and a rubric\n",
    "\n",
    "system_prompt = \"\"\"\\\n",
    "You are a student in a class. Your teacher has given you a homework assignment. Your task is to provide a response to the query which is of the specified quality (you can think of this as the grade). You will be provided with a query, a reference and a target score.\n",
    "\n",
    "The quality system is a score from 1 to 3, where 1 is bad, 2 is okay, and 3 is great. A reference answer is provided which by definition should score the maximum of 3. A rubric is also provided which describes the quality of the answer at each level.\n",
    "\n",
    "# Rubric\n",
    "1. The response is completely unhelpful. It is irrelevant, uninformative, potentially misdirecting or untruthful or it is a copy of the query. It may contain illogical or incoherent statements. It may also try to change the subject or be a non-sequitur. Reply with the query or a non-sequitur, or complete gibberish.\n",
    "2. The response is relevant and informative but contains factual inaccuracies or is incomplete. It may also contain some irrelevant information.\n",
    "3. By all measures the response is as good as the reference answer. It is relevant, informative, complete and accurate. It does not contain irrelevant information, nor does it contain any factual inaccuracies.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_template = \"\"\"\\\n",
    "# Query\n",
    "{query}\n",
    "\n",
    "# Reference Answer (scores 3)\n",
    "{reference}\n",
    "\n",
    "Produce a response which is characteristic of a score of {desired_score}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What factors contributed to the decline of the Seagull/Seamew owners association in the late 1970s and early 1980s, and is there currently an official association for these plywood boats?'\n",
    "\n",
    "challenge = 'What led to the disbandment of the Seagull/Seamew owners association in the late 1970s and early 1980s, and is there any active association for these plywood boats now?'\n",
    "\n",
    "reference = 'The decline of the Seagull/Seamew owners association in the late 1970s and early 1980s can be attributed to several factors. Firstly, the popularity of plywood boats, in general, began to wane as fiberglass construction methods became more prevalent. This led to a decrease in the number of plywood boats being built, which in turn resulted in fewer owners and less interest in the owners association. Secondly, the aging of the original owners also contributed to the decline of the association. Many of the owners who had built and sailed their Seagull or Seamew boats in the 1960s and 1970s had since moved on to other types of boats or retired from sailing altogether. As a result, there were fewer younger owners to take up the mantle and keep the association active.\\n\\nAs for whether there is currently an official association for Seagull and Seamew plywood boats, there is no such organization at present.'\n",
    "\n",
    "desired_score = 1\n",
    "\n",
    "user_prompt = user_prompt_template.format(query=query, reference=reference, desired_score=desired_score)\n",
    "\n",
    "messages = [\n",
    "    {'role':'system', 'content':system_prompt},\n",
    "    {'role':'user', 'content':user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Query\n",
      "What factors contributed to the decline of the Seagull/Seamew owners association in the late 1970s and early 1980s, and is there currently an official association for these plywood boats?\n",
      "\n",
      "# Reference Answer (scores 3)\n",
      "The decline of the Seagull/Seamew owners association in the late 1970s and early 1980s can be attributed to several factors. Firstly, the popularity of plywood boats, in general, began to wane as fiberglass construction methods became more prevalent. This led to a decrease in the number of plywood boats being built, which in turn resulted in fewer owners and less interest in the owners association. Secondly, the aging of the original owners also contributed to the decline of the association. Many of the owners who had built and sailed their Seagull or Seamew boats in the 1960s and 1970s had since moved on to other types of boats or retired from sailing altogether. As a result, there were fewer younger owners to take up the mantle and keep the association active.\n",
      "\n",
      "As for whether there is currently an official association for Seagull and Seamew plywood boats, there is no such organization at present.\n",
      "\n",
      "Produce a response which is characteristic of a score of 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = llm_pipeline.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = llm_pipeline(prompt, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = outputs[0][\"generated_text\"]\n",
    "print(response.replace(prompt, \"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
